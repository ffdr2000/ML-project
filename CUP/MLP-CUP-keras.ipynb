{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics \n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import keras\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "plt.rcParams.update({'font.size': 12})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUP MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing data\n",
    "path=r'/home/ludovico/ML-project/data/cup/ML-CUP23-'\n",
    "train_set = pd.read_csv(path+'TR.csv',skiprows=7, header=None, delimiter=',', dtype=str)\n",
    "\n",
    "input=train_set[train_set.columns[1:-3]]\n",
    "target=train_set[train_set.columns[-3:]]\n",
    "\n",
    "#splitting design set from test set (test set will be used only for the final model assessment)\n",
    "#the random seed is fixed to use the same design set for all the models\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(input, target, test_size=0.2, random_state=0, shuffle=True)\n",
    "\n",
    "x_train=x_train.astype(np.float64)\n",
    "y_train=y_train.astype(np.float64)\n",
    "\n",
    "#we add this metric (Mean euclidean error) to evaluate the performance of the model \n",
    "def MEE(x, y):\n",
    "    return np.mean(np.linalg.norm(x - y, 2, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating network with 1 layer using Keras \n",
    "We add 1 hidden layer with 10 (x_train.shape[1]) number of input units, dens_nparams number of units in the hidden layer, SGD as optimizer algorithm and 3 output unit with linear activation function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_model_1(init='uniform', \n",
    "                activation='relu',\n",
    "                nbr_features=10, \n",
    "                dense_nparams=10, \n",
    "                lr_init=0.1, \n",
    "                momentum=0.1, \n",
    "                w_d=0.0001, \n",
    "                nest=False,\n",
    "                decay_steps=100,\n",
    "                decay_rate=0.9):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(dense_nparams, activation, input_shape=(nbr_features,), kernel_initializer=init,)) \n",
    "    model.add(Dense(3, activation='linear', kernel_initializer=init))\n",
    "\n",
    "    lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    lr_init,\n",
    "    decay_steps=decay_steps,\n",
    "    decay_rate=decay_rate,\n",
    "    staircase=True)\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer=SGD(lr_schedule,momentum,nest,w_d))\n",
    "    return model\n",
    "\n",
    "#callback = keras.callbacks.EarlyStopping(monitor='loss',patience=3)\n",
    "keras_estimator = KerasRegressor(model=create_model_1,verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search for 1 layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.set_random_seed(42)\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'epochs': [1500],\n",
    "    'model__dense_nparams': [100],\n",
    "    'model__init': [ 'uniform' ], \n",
    "    'batch_size':[50],\n",
    "    'model__lr_init':[0.02],\n",
    "    'model__decay_steps':[1000],\n",
    "    'model__decay_rate':[0.90],\n",
    "    'model__momentum':[0.5],\n",
    "    'model__w_d':[0.001],\n",
    "    'model__activation':['tanh'],\n",
    "    'model__nest':[False]    \n",
    "}\n",
    "\n",
    "grid_search_1 = GridSearchCV(\n",
    "    estimator=keras_estimator,\n",
    "    param_grid=param_grid,\n",
    "    cv=RepeatedKFold(n_splits=5, n_repeats=5, random_state=0),\n",
    "    n_jobs=-1,\n",
    "    return_train_score = True,\n",
    "    refit=True,\n",
    "    scoring=make_scorer(MEE, greater_is_better=False),\n",
    "    verbose=3,\n",
    "    error_score='raise'\n",
    ")\n",
    "\n",
    "MLP_1=grid_search_1.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results_df = pd.DataFrame(MLP_1.cv_results_)\n",
    "best_model_index=MLP_1.best_index_\n",
    "\n",
    "print('best params',MLP_1.best_params_) \n",
    "\n",
    "val_loss=cv_results_df['mean_test_score'][best_model_index]\n",
    "val_std=cv_results_df['std_test_score'][best_model_index]\n",
    "train_loss=cv_results_df['mean_train_score'][best_model_index]\n",
    "train_std=cv_results_df['std_train_score'][best_model_index]\n",
    "print('Train loss:',train_loss,'+/-', train_std)\n",
    "print('Validation loss:',val_loss,'+/-', val_std)\n",
    "\n",
    "cv_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.set_random_seed(42)\n",
    "\n",
    "# Splitting the design set\n",
    "x_train_cl, x_val, y_train_cl, y_val = train_test_split(x_train, y_train, test_size=0.20, shuffle=True)\n",
    "\n",
    "model=KerasRegressor(model=create_model_1,**grid_search_1.best_params_)\n",
    "\n",
    "history = model.fit(x_train_cl, y_train_cl.astype(np.float64),validation_data=(x_val,y_val.astype(np.float64)))\n",
    "\n",
    "print(history.history_.keys())\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history_['loss'])\n",
    "plt.plot(history.history_['val_loss'], linestyle='--')\n",
    "\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.yscale('log')\n",
    "plt.legend(['Train', 'Validation'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating network with 2 layers using Keras \n",
    "We add 2 hidden layers with 10 (x_train.shape[1]) number of input units, dens_nparams number of units in the hidden layer, SGD as optimizer algorithm and 3 output unit with linear activation function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_2(init='uniform', \n",
    "                activation='relu', \n",
    "                nbr_features=10, \n",
    "                dense_nparams=10,\n",
    "                lr_init=0.1,\n",
    "                momentum=0.1,\n",
    "                w_d=0.0001, \n",
    "                nest=False,\n",
    "                decay_steps=100,\n",
    "                decay_rate=0.9):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(dense_nparams, activation, input_shape=(nbr_features,), kernel_initializer=init,)) \n",
    "    model.add(Dense(dense_nparams, activation, kernel_initializer=init))\n",
    "    model.add(Dense(3, activation='linear', kernel_initializer=init))\n",
    "\n",
    "    \n",
    "    lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    lr_init,\n",
    "    decay_steps=decay_steps,\n",
    "    decay_rate=decay_rate,\n",
    "    staircase=True)\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer=SGD(lr_schedule,momentum,nest,w_d))\n",
    "    return model\n",
    "\n",
    "keras_estimator = KerasRegressor(model=create_model_2,verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search for 2 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.set_random_seed(42)\n",
    "\n",
    "param_grid = {\n",
    "    'epochs': [3000],\n",
    "    'model__dense_nparams': [110],\n",
    "    'model__init': [ 'uniform' ], \n",
    "    'batch_size':[200],\n",
    "    'model__lr':[0.01],\n",
    "    'model__momentum':[0.8],\n",
    "    'model__w_d':[0.001],\n",
    "    'model__activation':['tanh'],\n",
    "    'model__nest':[True],\n",
    "    'model__decay_steps':[500]   \n",
    "}\n",
    "\n",
    "grid_search_2 = GridSearchCV(\n",
    "    estimator=keras_estimator,\n",
    "    param_grid=param_grid,\n",
    "    cv=RepeatedKFold(n_splits=5, n_repeats=5, random_state=0),\n",
    "    n_jobs=-1,\n",
    "    return_train_score = True,\n",
    "    refit=True,\n",
    "    scoring=make_scorer(MEE, greater_is_better=False),\n",
    "    verbose=3,\n",
    "    error_score='raise'\n",
    ")\n",
    "\n",
    "MLP_2=grid_search_2.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results_df = pd.DataFrame(MLP_2.cv_results_)\n",
    "best_model_index=MLP_2.best_index_\n",
    "\n",
    "print('best params',MLP_2.best_params_) \n",
    "\n",
    "val_loss=cv_results_df['mean_test_score'][best_model_index]\n",
    "val_std=cv_results_df['std_test_score'][best_model_index]\n",
    "train_loss=cv_results_df['mean_train_score'][best_model_index]\n",
    "train_std=cv_results_df['std_train_score'][best_model_index]\n",
    "print('Train loss:',train_loss,'+/-', train_std)\n",
    "print('Validation loss:',val_loss,'+/-', val_std)\n",
    "\n",
    "cv_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.set_random_seed(42)\n",
    "\n",
    "# Splitting the design set\n",
    "x_train_cl, x_val, y_train_cl, y_val = train_test_split(x_train, y_train, test_size=0.20, shuffle=True)\n",
    "\n",
    "model=KerasRegressor(model=create_model_2,**grid_search_2.best_params_)\n",
    "\n",
    "history = model.fit(x_train_cl, y_train_cl.astype(np.float64),validation_data=(x_val,y_val.astype(np.float64)))\n",
    "\n",
    "print(history.history_.keys())\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history_['loss'])\n",
    "plt.plot(history.history_['val_loss'], linestyle='--')\n",
    "\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.yscale('log')\n",
    "plt.legend(['Train', 'Validation'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The best model between 1 layer and 2 layers is manually selected as the one with the lowest MEE on validation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
